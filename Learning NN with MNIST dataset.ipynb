{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b90511-9544-40d0-9617-962faa834d82",
   "metadata": {},
   "source": [
    "# Learning NN with MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9ebff71-9f7b-4d9c-a996-c3e8f54a487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds # for importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "741e524a-45e3-494a-b77c-602bb6e9f039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow_core._api.v2.version' from '/opt/anaconda3/envs/py3-TF2.0/lib/python3.7/site-packages/tensorflow_core/_api/v2/version/__init__.py'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb610f6a-1f3e-4d47-8fc3-680ed478d66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 10:35:22.338906: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-01 10:35:22.342494: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "#load and preprocess the data\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name ='mnist', with_info ='true', as_supervised='true')\n",
    "# the method tfds.load() actually download the specify name='mnist' dataset into the local directory for the first time.\n",
    "# for later runs, the method draw the data locally.\n",
    "# the argument with_info = 'True' make the method return the info of the data as well. In this case I stored in the different variable called mnist_info\n",
    "# the argument as_supervised = 'True' make the data came with input-target format for easier uses with supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a124340-c35e-43b9-a524-75027d7de229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " 'train': <PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just checking\n",
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d52e66e-6683-4efe-b93d-b9d130bf43c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='/Users/noonpritsana/tensorflow_datasets/mnist/3.0.1',\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just checking\n",
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3bcd0b9-82c5-445d-8287-23f369d732c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from the info, we learned that the data are (28,28,1) image tensor call 'image' and label\n",
    "# also the 70000 observations has been splited into 60k train and 10k test observations\n",
    "mnist_train = mnist_dataset['train']\n",
    "mnist_test = mnist_dataset['test']\n",
    "\n",
    "# still we need a validation dataset for testing of the overfitting. We will draw that from the train dataset\n",
    "# let's slice 10% from the training set to the validation test using the 'num_examples' we have got from the dataset info\n",
    "num_validation_sample = 0.1* mnist_info.splits['train'].num_examples\n",
    "\n",
    "# Just in case that the 10% of the num_examples is a float (we need to split the observation with a gap between two observation)\n",
    "# Thus, cast a num_validation_sample into an int64 class\n",
    "num_validation_sample = tf.cast(num_validation_sample,tf.int64)\n",
    "\n",
    "# also defining the variable which contain the number of the test data \n",
    "# this time cast it to the tf.int64 at the same time\n",
    "num_test_sample = tf.cast(mnist_info.splits['test'].num_examples,tf.int64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de31d534-4684-4d4c-8b5e-2c948f80d4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=278, shape=(), dtype=int64, numpy=6000>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the amount of observations\n",
    "num_validation_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56fe575b-c773-4127-8fe8-98e07c7a6c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=280, shape=(), dtype=int64, numpy=10000>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the amount of observations\n",
    "num_test_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d317646-2227-43b6-bc60-700073a8704b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lets scale the data for better numarical stability\n",
    "\n",
    "The photo is actually the grayscale of a hand written digits in 28*28 pixcel. The grayscale is represented by a number 0 to 255 with 0 represent a total white and 255 a total black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20443eac-be68-4f16-916e-6869f40fd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the observation by dividing with 255 make a variable range from 0 to 1 instead of 0 to 255\n",
    "\n",
    "# we know that the dataset came with 2 part: the actual image and a label. \n",
    "# Thus to pass the dataset into the scale function, we need to pass both arguments.\n",
    "# Also the returns must have the same name as the arguments for furthur compatability \n",
    "\n",
    "def scale(image, label):\n",
    "    #to divided by 255, we need to make sure that the numerator is a float. Cast it.\n",
    "    image = tf.cast(image,tf.float64)\n",
    "    image = image/255\n",
    "    return image, label\n",
    "\n",
    "# By using the tf.map() method we can apply a certain function to the dataset. \n",
    "# Here we use .map() to apply scale() we have just defined\n",
    "#Here we scaled the entire train data. The valid will be extracted later.\n",
    "scaled_train_and_validation_data = mnist_train.map(scale) \n",
    "\n",
    "#do the same for the test\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f879d-f7be-480e-84d8-716379a16750",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Shuffle the data before splits into train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f15a8017-7f45-4bc7-abc2-a71671dab241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the amount of observations to be shuffle at once. \n",
    "# Without the buffer, if the dataset is too big, ram won't be enought to make a a shuffle.\n",
    "buffer = 10000 \n",
    "shuffled_train_and_test = scaled_train_and_validation_data.shuffle(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb29c9-46b5-4de9-b7ad-c1bf7f48f75f",
   "metadata": {},
   "source": [
    "### Actually spliting the data\n",
    "\n",
    "using tf.take() and tf.skip() to extract the bulk of data needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6843084-5d13-4410-89d6-300fe3fb5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = shuffled_train_and_test.take(num_validation_sample)\n",
    "train_data = shuffled_train_and_test.skip(num_validation_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f7cbc-61d1-4636-9280-bc2dbc313d1b",
   "metadata": {},
   "source": [
    "### Batching Data\n",
    "\n",
    "Batching is very beneficial in training since the process can be itereate from batches to batches. We would get the feedback from every iterations instead of only once form the entire dataset.\n",
    "\n",
    "** Noted that the batched iteration return the average loss and accuracy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11397ae6-0784-41c3-be75-44c025df95b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_data = train_data.batch(batch_size)\n",
    "\n",
    "# since the train data is in the batched form, the model expected the validation and test data tobe in batch form as well.\n",
    "# Therefore, we have to make 1 batch for both of the dataset by setting the batch_size with the number of its' observations.\n",
    "test_data = test_data.batch(num_test_sample)\n",
    "validation_data = validation_data.batch(num_validation_sample)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19528f-ea9c-40d8-8575-55c8dcbfa733",
   "metadata": {},
   "source": [
    "now the train /test / validation data are ready to be fed into the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4960e-0323-462c-b55e-26456bdb6c2f",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8413b59-1db5-47d7-b56a-31bc32b10f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "\n",
    "input_size = 28*28 #since the size of the drawing is 28*28 pixcel. We will flatten() it to become 28*28=784\n",
    "output_size = 10 #since we need 10 categiries from 0 to 9\n",
    "hidden_layer_size = 50 #arbitrary number\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            # flatten() tensor with mnist_info.features.shape['image'] (in this case (28,28,1)) into a vector.\n",
    "                            tf.keras.layers.Flatten(input_shape=mnist_info.features.shape['image']),\n",
    "                            # 1st hidden layer\n",
    "                            # relu return the actual possitive amount or else return 0\n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation='relu'), \n",
    "                            # 2nd hidden layer  \n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation='tanh'), \n",
    "                            # output layer\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                            #softmax return the probability of each output for the total of 1                            \n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374cba6-77cb-48b4-a756-c70cc818e675",
   "metadata": {},
   "source": [
    "### Picking the optimizer and loss function\n",
    "\n",
    "let's use **adam** for the optimizer since it's offer both stochastic and momentum property which leads to both speed and acceptable accuracy.\n",
    "\n",
    "let's use **sparse_categorical_crossentropy** for the loss function since we didn't one-hot encoded the data.\n",
    "\n",
    "Also, let's the metrics be **accuracy**\n",
    "\n",
    "We can state all three hyperparameters with the method .compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5e117ff-5ff7-410c-8b66-ff09a3145a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0250e-6d0b-4efe-8a16-8476698098fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c69b32d-775f-41da-b753-4e14841fa6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float64, tf.int64)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a55e4075-ae98-4788-a6a5-0b6bd55b30aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 7s - loss: 0.4057 - accuracy: 0.8904 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 11:10:30.120848: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "540/540 - 7s - loss: 0.1792 - accuracy: 0.9472 - val_loss: 0.1495 - val_accuracy: 0.9567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 11:10:37.222741: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "540/540 - 7s - loss: 0.1304 - accuracy: 0.9619 - val_loss: 0.1149 - val_accuracy: 0.9670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 11:10:43.916070: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "540/540 - 7s - loss: 0.1050 - accuracy: 0.9693 - val_loss: 0.1038 - val_accuracy: 0.9692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 11:10:50.628494: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "540/540 - 7s - loss: 0.0861 - accuracy: 0.9748 - val_loss: 0.0926 - val_accuracy: 0.9718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 11:10:57.495729: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "540/540 - 7s - loss: 0.0754 - accuracy: 0.9776 - val_loss: 0.0825 - val_accuracy: 0.9763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 11:11:04.366229: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "540/540 - 7s - loss: 0.0636 - accuracy: 0.9814 - val_loss: 0.0666 - val_accuracy: 0.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 11:11:10.898194: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "540/540 - 7s - loss: 0.0563 - accuracy: 0.9837 - val_loss: 0.0635 - val_accuracy: 0.9815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 11:11:17.473250: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "540/540 - 6s - loss: 0.0482 - accuracy: 0.9857 - val_loss: 0.0559 - val_accuracy: 0.9847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 11:11:23.918273: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "540/540 - 6s - loss: 0.0418 - accuracy: 0.9879 - val_loss: 0.0536 - val_accuracy: 0.9845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 11:11:30.366675: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa993f66ed0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the hyperparameter\n",
    "n_epochs = 10\n",
    "\n",
    "#fit the model\n",
    "model.fit(x=train_data, epochs=n_epochs, validation_data=(validation_inputs,validation_targets),validation_steps=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96cba5-04b3-45b2-a816-90032058a43b",
   "metadata": {},
   "source": [
    "### Tuning the hyperparameters\n",
    "\n",
    "This is the exercise from the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41540bb2-65ba-4ffd-a251-cea247c2d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the hidden layers to 200: increasing the width of the model. -> Longer training time + better accuracy\n",
    "# addding another layer of the hidden layer: increasing the depth of themodel. -> Longer training time\n",
    "# adding both width and depth size = 200 and layer = 5. -> longer training time + better accuracy\n",
    "# try Sigmoid activation function instead of RELU: -> longer training time + worse accuracy\n",
    "# try ReLu for the first hidden and tanh activation function for the second ->  fast and accurate \n",
    "# adjust the batch size to 10000 from 100 -> very fast and accurate \n",
    "# adjust the batch size to 1 (this is the Simple gradient descent) -> this takes so much time\n",
    "# adjust the learning rate to 0.0001% this takes longer time since the epoch goes up but for the first five epochs the accuracy is better already\n",
    "# adjust the learning rate to 0.02% -> fast and accurate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5daf517-f3c2-4adf-9131-a653f8ff555f",
   "metadata": {},
   "source": [
    "### Actually testing the modle\n",
    "\n",
    "The accuracy and loss function from those earlier epochs are form the \"average loss and accuracy\" from the test batches and from validation data.(we are overfitting the validation data) We are not getting any real testing accuracy and loss yet. Let's do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "609f21e7-062b-4abd-b757-ff75fb6bdde6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Cache should only be read after it has been completed. [Op:MakeIterator]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gn/myx711092bl2khdlzkhf4bk40000gn/T/ipykernel_1320/3867435486.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_loss = {} : test_accuracy = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py3-TF2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m   def predict(self,\n",
      "\u001b[0;32m/opt/anaconda3/envs/py3-TF2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     return self._model_iteration(\n\u001b[1;32m    455\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         sample_weight=sample_weight, steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[0;32m/opt/anaconda3/envs/py3-TF2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m           model, mode)\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m       callbacks = cbks.configure_callbacks(\n",
      "\u001b[0;32m/opt/anaconda3/envs/py3-TF2.0/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m     if (context.executing_eagerly()\n\u001b[1;32m    331\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[0;32m--> 332\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m/opt/anaconda3/envs/py3-TF2.0/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    591\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/py3-TF2.0/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    609\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m/opt/anaconda3/envs/py3-TF2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2927\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2928\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2929\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2930\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2931\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n",
      "\u001b[0;32m/opt/anaconda3/envs/py3-TF2.0/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Cache should only be read after it has been completed. [Op:MakeIterator]"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print('test_loss = {} : test_accuracy = {}'.format(test_loss,test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb1365-3ffd-45b5-aa94-37ba25d308e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
